{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction with Anthropic Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this example, you are going to ingest text representing description of New York City (using a csv obtained from the train split of the [`rajpurkar/squad`](https://huggingface.co/datasets/rajpurkar/squad) dataset of HuggingFace) directly into Amazon Bedrock API (using Anthropic Claude model) and give it an instruction to extract information from it.\n",
    "\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. Download the csv (containing text, along with questions and ground truth data extracted from it)\n",
    "1. Use this text as input data for the model\n",
    "1. The foundation model processes the input data\n",
    "1. Model returns a response with the data extracted from the ingested text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\" \\\n",
    "    \"anthropic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "\n",
    "module_path = \"../..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York City small dataset\n",
    "It is a dataset of informations about NYC, with associated questions and answers: this allows us to get feedback on the correctness of our extractions.\\\n",
    "We ingest the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"../data/nyc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can leverage the structure of the dataset to test Claude's ability in information extraction\n",
    "\n",
    "Let's select a subset of the DataFrame rows and for each of them let's use the question field as a prompt. Then, we can compare the answer given by the model with the one available on the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short=df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we will have to iterate over the rows and perform a call to the model in each iteration. It's useful to build a function that accepts the question and the text as inputs, performs the request to the model and returns the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_model(context,question):\n",
    "    prompt_data=f\"\\n\\nHuman:Answer the following question using the data read in the text.\\nQuestion: {question}\\nText:{context}\\n\\nAssistant:\"\n",
    "    try:\n",
    "\n",
    "        body = json.dumps({\"prompt\": prompt_data,\"max_tokens_to_sample\":1024})\n",
    "        modelId = \"anthropic.claude-v2\"\n",
    "        accept = \"application/json\"\n",
    "        contentType = \"application/json\"\n",
    "\n",
    "        response = boto3_bedrock.invoke_model(\n",
    "            body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "        )\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body.get(\"completion\")\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "\n",
    "        if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "            print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                    \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                    \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                    \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "        else:\n",
    "            raise error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the following loop to attach the model answers to the shortened DataFrame, in order to confront them easily with the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_short.iterrows():\n",
    "    df_short.loc[index,\"llm_answer\"]=request_to_model(row[\"context\"],row[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df_short.iterrows():\n",
    "    print(f\"Ground truth: {row['answers']}, LLM output: {row['llm_answer']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the LLM gives out pretty long answers, what if we would want the most concise answer possible (similar to the ground truth ones)? Let's try modifying the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_model_concise(context,question):\n",
    "    prompt_data=f\"\\n\\nHuman:Answer the following question using the data read in the text.\\nQuestion: {question}\\nText:{context}\\nJust provide the information, without mentioning what you read in the text.\\n\\nAssistant:\"\n",
    "    try:\n",
    "\n",
    "        body = json.dumps({\"prompt\": prompt_data,\"max_tokens_to_sample\":1024})\n",
    "        modelId = \"anthropic.claude-v2\"\n",
    "        accept = \"application/json\"\n",
    "        contentType = \"application/json\"\n",
    "\n",
    "        response = boto3_bedrock.invoke_model(\n",
    "            body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "        )\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body.get(\"completion\")\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "\n",
    "        if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "            print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                    \\nTo troubeshoot this issue please refer to the following resources.\\\n",
    "                    \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                    \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "\n",
    "        else:\n",
    "            raise error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test again, executing the same loop to get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_short.iterrows():\n",
    "    df_short.loc[index,\"llm_answer\"]=request_to_model_concise(row[\"context\"],row[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df_short.iterrows():\n",
    "    print(f\"Ground truth: {row['answers']}, LLM output: {row['llm_answer']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by slightly changing the prompt we obtained results that are similar to those of the ground truth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, you try experimenting with Information Extraction! In the 'data' folder there is also another small dataset with text talking about Buddhism. \n",
    "### Try experimenting with different prompts, you could get even better output formats using the data as examples for Few-Shot or CoT prompting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
