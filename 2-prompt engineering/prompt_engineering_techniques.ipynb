{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIRED DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  \\\n",
    "    \"langchain>=0.0.350\" \\\n",
    "    \"transformers>=4.24,<5\" \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESTART THE KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT BEDROCK CLIENT\n",
    "\n",
    "Establishing a connection to the Berock service using AWS credentials and configuration settings provided through environment variables. This allows the Python script to interact with the Bedrock service using the boto3_bedrock object, which serves as a client for making API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SET UP THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "    inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                        \"temperature\":0.5,\n",
    "                        \"top_k\":250,\n",
    "                        \"top_p\":1,\n",
    "                        \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                        }\n",
    "\n",
    "    textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                        client = boto3_bedrock, \n",
    "                        model_kwargs = inference_modifier \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting engineering\n",
    "Prompting engineerins is the process of designing prompts in a way that optimizes the performance of large language models (LLMs) for specific tasks.\n",
    "In this practice we are going to use the following prompting techniques :\n",
    "- Zero-shot\n",
    "- Few-shot\n",
    "- COT\n",
    "\n",
    "## Zero-shot prompt templates\n",
    "Zero-shot prompt engineering involves crafting prompts for language models to enable them to perform tasks without explicit training examples or fine-tuning on that specific task, leveraging their initial training.\n",
    "\n",
    "Large language models (LLMs) excel at various tasks out of the box:\n",
    "- Text Generation: Generating text in a specific style or genre without fine-tuning on a large corpus of text in that style.\n",
    "- Text Classification: Classifying text into predefined categories without explicit training examples for each category.\n",
    "- Question Answering: Answering questions based on general knowledge without task-specific training data.\n",
    "- Language Translation: Translating text between languages without training data for specific language pairs.\n",
    "- Summarization: Generating summaries of text documents without task-specific training on summarization datasets.\n",
    "- Sentiment Analysis: Analyzing the sentiment of text without labeled examples for each sentiment class.\n",
    "\n",
    "Today's large LLMs, such as GPT-3.5, are finely tuned to follow instructions and are trained on vast amounts of data, making them capable of performing various tasks \"zero-shot.\"\n",
    "\n",
    "Here is one example of the Zero-shot techniques used for Sentiment Analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Classify the text into neutral, negative or positive.\\n\" \n",
    "    \"Text: I think the vacation is okay.\\n\"\n",
    "    \"Sentiment:\"\n",
    ")\n",
    "\n",
    "response = textgen_llm(prompt_template.template)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the prompt above we didn't provide the model with any examples of text alongside their classifications, the LLM already understands \"sentiment\" -- that's the zero-shot capabilities at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises :\n",
    "Try to solve the following problem using the Zero-shot techniques :\n",
    "\n",
    "#### Exercise 1 - Text Summarization:\n",
    "One of the standard tasks in natural language generation is text summarization. Text summarization can include many different flavors and domains. In fact, one of the most promising applications of language models is the ability to summarize articles and concepts into quick and easy-to-read summaries. Let's try a basic summarization task using prompts.\n",
    "\n",
    "Let's say you are interested to learn about antibiotics.\n",
    "\n",
    "Start by requesting an explanation of antibiotics from the model.\n",
    "Then, ask the model to provide a concise summary of the definition it provided earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the definition of antibiotics\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Explain antibiotics\\n\"\n",
    "    \"A:\"\n",
    ")\n",
    "antibiotics_definition = textgen_llm(prompt_template.template)\n",
    "\n",
    "#Summarize the definition of antibiotics\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"{text_to_summarize}\\n\"\n",
    "    \"explain the above text in one sentence :\"\n",
    ")\n",
    "prompt_template.format(text_to_summarize=antibiotics_definition)\n",
    "summary = textgen_llm(prompt_template.template)\n",
    "\n",
    "print_ww(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - Code generation :\n",
    "One application where LLMs are quite effective is code generation. Copilot is a great example of this. There are a vast number of code-generation tasks you can perform with clever prompts.\n",
    "\n",
    "Imagine you're conducting an analysis across various departments within the university.\n",
    "You need to extract the studentsID and the students names of the computer science department.\n",
    "\n",
    "Provide data about the database schema crafting a good prompt.\n",
    "Then ask the model to generate a valid MySQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the definition of antibiotics\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Table departments, columns = [DepartmentId, DepartmentName]\n",
    "    Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "    Create a MySQL query for all students in the Computer Science Department\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "SQL_query = textgen_llm(prompt_template.template)\n",
    "\n",
    "print_ww(SQL_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompt templates\n",
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\n",
    "\n",
    "Let's demonstrate few-shot prompting via an example that was presented in [Brown et al. 2020](https://arxiv.org/abs/2005.14165)\n",
    ". In the example, the task is to correctly use a new word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "        A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "        We were traveling in Africa and we saw these very cute whatpus.\n",
    "    \n",
    "        To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "response = textgen_llm(prompt_template.template)\n",
    "\n",
    "print_ww(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output the model understands the task and creates a valid example of use for an invented word.\n",
    "\n",
    "Let's see another example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "   \"\"\"\n",
    "      This is awesome! // Negative\n",
    "      This is bad! // Positive\n",
    "      Wow that movie was rad! // Positive\n",
    "      What a horrible show! //\n",
    "   \"\"\"\n",
    ")\n",
    "\n",
    "response = textgen_llm(prompt_template.template)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Reasoning:\n",
    "Perhaps one of the most difficult tasks for an LLM today is one that requires some form of reasoning. Reasoning is one of most interesting areas due to the types of complex applications that can emerge from LLMs.\n",
    "\n",
    "There have been some improvements in tasks involving mathematical capabilities. That said, it's important to note that current LLMs struggle to perform reasoning tasks so this requires even more advanced prompt engineering techniques. We will cover these advanced techniques in the next guide. For now, we will cover a few basic examples to show arithmetic capabilities.\n",
    "\n",
    "Given the prompt :\n",
    "\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\n",
    "\n",
    "Check if the LLM can provide an accurate answer. If not, refine the prompt to assist the LLM in completing the task successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-working solution with Few-shot technique\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "   \"\"\"\n",
    "      The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "      A: The answer is False.\n",
    "      The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "      A: The answer is True.\n",
    "      The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "      A: The answer is True.\n",
    "      The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "      A: The answer is False.\n",
    "      The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "      A: \n",
    "   \"\"\"\n",
    ")\n",
    "\n",
    "response = textgen_llm(prompt_template.template)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COT : Chain of thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot COT : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
