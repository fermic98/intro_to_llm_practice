{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIRED DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  \\\n",
    "    \"langchain>=0.0.350\" \\\n",
    "    \"transformers>=4.24,<5\" \\\n",
    "    sqlalchemy -U \\\n",
    "    \"faiss-cpu>=1.7,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESTART THE KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT BEDROCK CLIENT\n",
    "\n",
    "Establishing a connection to the Berock service using AWS credentials and configuration settings provided through environment variables. This allows the Python script to interact with the Bedrock service using the boto3_bedrock object, which serves as a client for making API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SET UP THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "    inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                        \"temperature\":0.5,\n",
    "                        \"top_k\":250,\n",
    "                        \"top_p\":1,\n",
    "                        \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                        }\n",
    "\n",
    "    textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                        client = boto3_bedrock, \n",
    "                        model_kwargs = inference_modifier \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting engineering\n",
    "Prompting engineerins is the process of designing prompts in a way that optimizes the performance of large language models (LLMs) for specific tasks.\n",
    "In this practice we are going to use the following prompting techniques :\n",
    "- Zero-shot\n",
    "- Few-shot\n",
    "- COT\n",
    "\n",
    "## Zero-shot prompt templates\n",
    "Zero-shot prompt engineering involves crafting prompts for language models to enable them to perform tasks without explicit training examples or fine-tuning on that specific task, leveraging their initial training.\n",
    "\n",
    "Large language models (LLMs) excel at various tasks out of the box:\n",
    "- Text Generation: Generating text in a specific style or genre without fine-tuning on a large corpus of text in that style.\n",
    "- Text Classification: Classifying text into predefined categories without explicit training examples for each category.\n",
    "- Question Answering: Answering questions based on general knowledge without task-specific training data.\n",
    "- Language Translation: Translating text between languages without training data for specific language pairs.\n",
    "- Summarization: Generating summaries of text documents without task-specific training on summarization datasets.\n",
    "- Sentiment Analysis: Analyzing the sentiment of text without labeled examples for each sentiment class.\n",
    "\n",
    "Today's large LLMs, such as GPT-3.5, are finely tuned to follow instructions and are trained on vast amounts of data, making them capable of performing various tasks \"zero-shot.\"\n",
    "\n",
    "Here is one example of the Zero-shot techniques used for Sentiment Analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Classify the text into neutral, negative or positive. \n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the prompt above we didn't provide the model with any examples of text alongside their classifications, the LLM already understands \"sentiment\" -- that's the zero-shot capabilities at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises :\n",
    "Try to solve the following problem using the Zero-shot techniques :\n",
    "\n",
    "#### Exercise 1 - Text Summarization:\n",
    "One of the standard tasks in natural language generation is text summarization. Text summarization can include many different flavors and domains. In fact, one of the most promising applications of language models is the ability to summarize articles and concepts into quick and easy-to-read summaries. Let's try a basic summarization task using prompts.\n",
    "\n",
    "Let's say you are interested to learn about antibiotics.\n",
    "\n",
    "Start by requesting an explanation of antibiotics from the model.\n",
    "Then, ask the model to provide a concise summary of the definition it provided earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the definition of antibiotics\n",
    "prompt = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "antibiotics_definition = textgen_llm(prompt)\n",
    "\n",
    "#Summarize the definition of antibiotics\n",
    "prompt_2 = antibiotics_definition + \"...\"\n",
    "summary = textgen_llm(prompt_2)\n",
    "\n",
    "print_ww(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - Code generation :\n",
    "One application where LLMs are quite effective is code generation. Copilot is a great example of this. There are a vast number of code-generation tasks you can perform with clever prompts.\n",
    "\n",
    "Imagine you're conducting an analysis across various departments within the university.\n",
    "You need to extract the studentsID and the students names of the computer science department.\n",
    "\n",
    "Provide info about possible database tables the model should use in the query.\n",
    "Then ask the model to generate a valid MySQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the definition of antibiotics\n",
    "prompt = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "SQL_query = textgen_llm(prompt)\n",
    "\n",
    "print_ww(SQL_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompt templates\n",
    "While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\n",
    "\n",
    "Let's demonstrate few-shot prompting via an example that was presented in [Brown et al. 2020](https://arxiv.org/abs/2005.14165)\n",
    ". In the example, the task is to correctly use a new word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output the model understands the task and creates a valid example of use for an invented word.\n",
    "\n",
    "Let's see another example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"\"\"\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise - Reasoning:\n",
    "Perhaps one of the most difficult tasks for an LLM today is one that requires some form of reasoning. Reasoning is one of most interesting areas due to the types of complex applications that can emerge from LLMs.\n",
    "\n",
    "There have been some improvements in tasks involving mathematical capabilities. That said, it's important to note that current LLMs struggle to perform reasoning tasks so this requires even more advanced prompt engineering techniques. \n",
    "\n",
    "Given the prompt :\n",
    "\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\n",
    "The model should answer with True, if the statement is correct, False otherwise.\n",
    "\n",
    "Check if the LLM can provide an accurate answer. If not, refine the prompt to assist the LLM in completing the task successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-working solution with Few-shot technique\n",
    "prompt = \"\"\"\n",
    "\n",
    "...\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COT : Chain of thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduced in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a perfect result when we provided the reasoning step. In fact, we can solve this task by providing even fewer examples, i.e., just one example seems enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - Math problem :\n",
    "The next exercise present a prompt based on the Few-shot prompting technique. Since it involves reasoning it does not perform well.\n",
    "\n",
    "Run the model with the given prompt to observe the answer and then try to improve it by using the COT technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example with Few-shot prompting\n",
    "prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does he have now?\n",
    "\n",
    "A: The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)\n",
    "\n",
    "#Correct solution\n",
    "prompt = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - Commonsense:\n",
    "\n",
    "Another area where COT can excel is in addressing common sense questions.\n",
    "Change the prompt with few-shot COT technique and try to make the model answer correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "...\n",
    "\n",
    "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\n",
    "states (b) mexico (c) countryside (d) atlas\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 - StrategyQA:\n",
    "\n",
    "Another area where COT can excel is in addressing strategcal reasoning question.\n",
    "Change the prompt with few-shot COT technique and try to make the model answer correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "...\n",
    "\n",
    "Q: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 - Sports understanding:\n",
    "\n",
    "Another area where COT can excebe exploited is in make the model understand terminologies of a specific field\n",
    "that may commonly have other meanings.\n",
    "Change the prompt with few-shot COT technique and try to make the model answer correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "...\n",
    "\n",
    "Q: Is the following sentence plausible? “Draymond Green threw a touchdown.”\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 - Last letter concatenation:\n",
    "\n",
    "Another area where COT can excel is in task concerning carachters manipulation.\n",
    "Change the prompt with few-shot COT technique and try to make the model answer correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Q: Take the last letters of the words\n",
    "in “Lady Gaga” and concatenate\n",
    "them.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot COT : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One recent idea that came out more recently is the idea of zero-shot CoT [(Kojima et al. 2022)]() that essentially involves adding \"Let's think step by step\" to the original prompt. Let's try a simple problem and see how the model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt with Zero-shot COT\n",
    "prompt = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.\n",
    "I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "Let's think step by step.\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------\n",
    "                                    SEE BELOW FOR EXERCISES SOLUTIONS\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTIONS\n",
    "\n",
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "\n",
    "#Get the definition of antibiotics\n",
    "prompt = \"\"\"\n",
    "\"Explain antibiotics\\n\"\n",
    "\"A:\"\n",
    "\"\"\"\n",
    "\n",
    "antibiotics_definition = textgen_llm(prompt)\n",
    "\n",
    "#Summarize the definition of antibiotics\n",
    "prompt_2 = antibiotics_definition + \"\\nExplain the above text in one sentence :\"\n",
    "summary = textgen_llm(prompt_2)\n",
    "\n",
    "print_ww(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Exercise 2\n",
    "\n",
    "#Get the definition of antibiotics\n",
    "prompt = \"\"\"\n",
    "    Table departments, columns = [DepartmentId, DepartmentName]\n",
    "    Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "    Create a MySQL query for all students in the Computer Science Department\n",
    "    \"\"\"\n",
    "\n",
    "SQL_query = textgen_llm(prompt)\n",
    "\n",
    "print_ww(SQL_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Exercise 1\n",
    "#Solution with Few-shot technique\n",
    "prompt = \"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COT - Chain of thaugths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Exercise 1\n",
    "\n",
    "#Example with Few-shot prompting\n",
    "prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2\n",
    "prompt = \"\"\"\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet\n",
    "A: The answer must require cable. Of the above choices, only television requires cable. So the answer is (c).\n",
    "\n",
    "Q: The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty flowers (b)\n",
    "hen house (c) natural habitat (d) storybook\n",
    "A: The answer must be something in the forest. Of the above choices, only natural habitat is in the forest. So the\n",
    "answer is (b).\n",
    "\n",
    "Q: Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\n",
    "states (b) mexico (c) countryside (d) atlas\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 3\n",
    "prompt = \"\"\"\n",
    "Q: Yes or no: Is it common to see frost during some college commencements?\n",
    "A: College commencement ceremonies can happen in December, May, and June. December is in the winter, so\n",
    "there can be frost. Thus, there could be frost at some commencements. So the answer is yes.\n",
    "\n",
    "Q: Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\n",
    "A: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6\n",
    "months. Thus, a llama could not give birth twice during the War in Vietnam. So the answer is no.\n",
    "\n",
    "Q: Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 4\n",
    "prompt = \"\"\"\n",
    "Q: Is the following sentence plausible? “Kyle Palmieri was called for slashing.”\n",
    "A: Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. So the answer is yes.\n",
    "\n",
    "Q: Is the following sentence plausible? “Carson Wentz set the pick and roll.”\n",
    "A: Carson Wentz is an American football player. Pick and roll is part of basketball, not football. So the answer\n",
    "is no.\n",
    "\n",
    "Q: Is the following sentence plausible? “Draymond Green threw a touchdown.”\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 5\n",
    "prompt = \"\"\"\n",
    "Q: Take the last letters of the words in \"Elon Musk\" and concatenate them.\n",
    "A: The last letter of \"Elon\" is \"n\". The last letter of \"Musk\" is \"k\". Concatenating them is \"nk\". The answer is nk.\n",
    "\n",
    "Q: Take the last letters of the words in \"Larry Page\" and concatenate them.\n",
    "A: The last letter of \"Larry\" is \"y\". The last letter of \"Page\" is \"e\". Concatenating them is \"ye\". The answer is ye\n",
    "\n",
    "Q: Take the last letters of the words\n",
    "in “Lady Gaga” and concatenate\n",
    "them.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
